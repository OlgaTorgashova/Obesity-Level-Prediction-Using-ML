{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64b74c0-e55e-44d7-af0c-bef972b9f807",
   "metadata": {},
   "source": [
    "## Graphical Visualization for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e75941-53fa-4005-9038-3b91f9c87b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_hist(row, col, ind, dataset, feature, str_feature, bins, color):\n",
    "    plt.subplot(row, col, ind)\n",
    "    plt.hist(dataset[feature], bins = bins, color = color, edgecolor = 'black')\n",
    "    plt.title(str_feature + ' Distribution')\n",
    "    plt.xlabel(str_feature)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "def next_scatter(row, col, ind, x, y, hue, str_x, str_y, legend_adjustment):\n",
    "    plt.subplot(row, col, ind)\n",
    "    sns.scatterplot(x = x, y = y, hue = hue, palette = 'viridis')\n",
    "    plt.xlabel(str_x)\n",
    "    plt.ylabel(str_y)\n",
    "    plt.title('Scatter Plot: ' + str_y + ' vs. ' + str_x)\n",
    "    plt.legend(title = 'Obes/No', loc = legend_adjustment, fancybox = True, framealpha = 0.7)\n",
    "    plt.grid(True)\n",
    "\n",
    "def next_hist_multi(row, col, ind, dataset, multiplied_df, feature, str_feature, k, colors, labels):\n",
    "    plt.subplot(row, col, ind)\n",
    "    bins = np.linspace(dataset[feature].min(), dataset[feature].max(), num = k + 1)\n",
    "    plt.hist(multiplied_df, bins = bins, color = colors, edgecolor = 'black', label = labels)\n",
    "    plt.legend(prop={'size': 8})\n",
    "    plt.title(str_feature + ' Distribution')\n",
    "    plt.xlabel(str_feature)\n",
    "    plt.grid(True)\n",
    "\n",
    "def get_hot_encoder(dataset):\n",
    "    NObeyesdad_cat = dataset[['NObeyesdad']]\n",
    "    cat_encoder = OneHotEncoder()\n",
    "    NObeyesdad_cat_1hot = cat_encoder.fit_transform(NObeyesdad_cat)\n",
    "    return cat_encoder, NObeyesdad_cat_1hot\n",
    "\n",
    "def multiply_feature_by_one_hot(dataset, feature_name, cat_encoder, NObeyesdad_cat_1hot):\n",
    "    # Extract the feature from dataset\n",
    "    feature = dataset[feature_name].values.reshape(-1, 1)  # Reshape to a column vector\n",
    "    # Perform element-wise multiplication\n",
    "    multiplied = NObeyesdad_cat_1hot.multiply(feature)\n",
    "    # Convert the result to a DataFrame\n",
    "    multiplied_df = pd.DataFrame()\n",
    "    multiplied_df = pd.DataFrame(multiplied.toarray(), columns = cat_encoder.get_feature_names_out(['NObeyesdad']))\n",
    "    return(multiplied_df)\n",
    "\n",
    "def next_boxplot_multi(row, col, ind, multiplied_df, feature, str_feature, labels):\n",
    "    data_vectors = []\n",
    "    for col_ind in multiplied_df.columns:\n",
    "        col_df = multiplied_df[col_ind][multiplied_df[col_ind] != 0]\n",
    "        data_vectors.append(np.array(col_df))\n",
    "    plt.subplot(row, col, ind)\n",
    "    plt.boxplot(data_vectors, labels = labels)\n",
    "    plt.title('Data Distribution over ' + str_feature)\n",
    "    plt.grid(True)\n",
    "    \n",
    "def get_hot_encoder(dataset):\n",
    "    NObeyesdad_cat = dataset[['NObeyesdad']]\n",
    "    cat_encoder = OneHotEncoder()\n",
    "    NObeyesdad_cat_1hot = cat_encoder.fit_transform(NObeyesdad_cat)\n",
    "    return cat_encoder, NObeyesdad_cat_1hot\n",
    "\n",
    "def multiply_feature_by_one_hot(dataset, feature_name, cat_encoder, NObeyesdad_cat_1hot):\n",
    "    # Extract the feature from dataset\n",
    "    feature = dataset[feature_name].values.reshape(-1, 1)  # Reshape to a column vector\n",
    "    # Perform element-wise multiplication\n",
    "    multiplied = NObeyesdad_cat_1hot.multiply(feature)\n",
    "    # Convert the result to a DataFrame\n",
    "    multiplied_df = pd.DataFrame()\n",
    "    multiplied_df = pd.DataFrame(multiplied.toarray(), columns = cat_encoder.get_feature_names_out(['NObeyesdad']))\n",
    "    return(multiplied_df)\n",
    "\n",
    "def box_plot_zoom(df, feature_y, feature_x, labels, title):\n",
    "    data_vectors = list()\n",
    "    unique_values = sorted(pd.unique(df[feature_x]))\n",
    "    for i in unique_values:\n",
    "        df_i = public_trans[feature_y][df[feature_x] == i]\n",
    "        data_vectors.append(np.array(df_i))\n",
    "    plt.boxplot(data_vectors, labels = labels)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe9d43-b5fb-456c-afed-9996c91092ac",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5ab784-c772-4c62-bb3c-f3c021379500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for fitting the models\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def LR_fitting(X_train, y_train, hyper_C, max_iter):\n",
    "    \"\"\"\n",
    "    The function creates and fits a logistic regression model.\n",
    "\n",
    "    Args:\n",
    "        X_train: A train feature matrix.\n",
    "        y_train: A train output vector.\n",
    "        hyper_C: A hyperparameter C which is used in the denominator in the penalty term.\n",
    "        max_iter: A hyperparameter - maximum number of iterations. \n",
    "\n",
    "    Returns:\n",
    "        The fitted model.\n",
    "    \"\"\"\n",
    "    model = LogisticRegression(C = hyper_C, multi_class = 'multinomial', solver = 'lbfgs', max_iter = max_iter)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def KNN_fitting(X_train, y_train, n_neighbors = 3):\n",
    "    \"\"\"\n",
    "    The function creates and fits a KNN model.\n",
    "\n",
    "    Args:\n",
    "        X_train: A train feature matrix.\n",
    "        y_train: A train output vector.\n",
    "        n_neighbors: A hyperparameter - a number of neighbors.\n",
    "\n",
    "    Returns:\n",
    "        The fitted model.\n",
    "    \"\"\"\n",
    "    model = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def DT_fitting(X_train, y_train, min_samples_leaf, max_depth):\n",
    "    \"\"\"\n",
    "    The function creates and fits a KNN model.\n",
    "\n",
    "    Args:\n",
    "        X_train: A train feature matrix.\n",
    "        y_train: A train output vector.\n",
    "        min_samples_leaf: A hyperparameter - minimum number of samples required to split an internal node.\n",
    "        max_depth: A hyperparameter - maximum depth of the tree.\n",
    "\n",
    "    Returns:\n",
    "        The fitted model.\n",
    "    \"\"\"\n",
    "    model = DecisionTreeClassifier(criterion = 'entropy', min_samples_leaf = min_samples_leaf, max_depth = max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def SVM_fitting(X_train, y_train, hyper_C):\n",
    "    \"\"\"\n",
    "    The function creates and fits a support vector classificator with the linear kernel.\n",
    "\n",
    "    Args:\n",
    "        X_train: A train feature matrix.\n",
    "        y_train: A train output vector.\n",
    "        hyper_C: A hyperparameter C which is inverse proportional to a regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "        The fitted model.\n",
    "    \"\"\"\n",
    "    model = SVC(C = hyper_C, kernel = 'linear')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "099d5f21-475e-4c22-b0b9-b3d4a77d59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_choice(X_train_i, y_train, model_name, dict_of_params):\n",
    "    \"\"\"\n",
    "    The function chooses the model depending on model_name.\n",
    "\n",
    "    Args:\n",
    "        X_train_i: A train feature matrix. \n",
    "        y_train: A train output vector.\n",
    "        model_name: A model name.\n",
    "        dict_of_params: A dictionary containing hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        The fitted model.\n",
    "    \"\"\"\n",
    "    if model_name   == 'LR':\n",
    "        iters       = dict_of_params['max_iter']\n",
    "        hyper_C     = dict_of_params['C']\n",
    "        model       = LR_fitting(X_train_i, y_train, hyper_C, iters)\n",
    "    elif model_name == 'KNN':\n",
    "        n_neighbors = dict_of_params['n_neighbors']\n",
    "        model       = KNN_fitting(X_train_i, y_train, n_neighbors)\n",
    "    elif model_name == 'DT':\n",
    "        min_samples_leaf = dict_of_params['min_samples_leaf']\n",
    "        max_depth   = dict_of_params['max_depth']\n",
    "        model       = DT_fitting(X_train_i, y_train, min_samples_leaf, max_depth)\n",
    "    elif model_name == 'SVC':\n",
    "        hyper_C     = dict_of_params['C']\n",
    "        model       = SVM_fitting(X_train_i, y_train, hyper_C)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f89b85-743b-43ae-a76d-f35803f8e714",
   "metadata": {},
   "source": [
    "## Feature Extraction Using Backward Stepwise Algorithm\n",
    "\r\n",
    "In this section, the backward stepwise algorithm is used for constructing performance metrics in the space ofa numberr of features and a chosen hyperparameter. In KNN the number of neighbors is used as a hyperparameter. Inthe  Decision Tree depth of a tree is used as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73843367-a6c8-4d8e-9688-eb912912e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(X_train_scaled, y_train, X_test_scaled, y_test, model_name, dict_of_params):\n",
    "    \"\"\"\n",
    "    The function extracts features using the backward stepwise algorithm.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled: A scaled train feature matrix.\n",
    "        y_train:        A train output vector. \n",
    "        X_test_scaled:  A scaled test feature matrix. \n",
    "        y_test:         A test output vector.\n",
    "        model_name:     A model name.\n",
    "        dict_of_params: A dictionary containing hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        df_train: A data frame that contains the following columns\n",
    "                 'acc' accuracy of the best among the models containing a reduced number of features\n",
    "                 'f1'  f1 score of the best among the models containing a reduced number of features\n",
    "                 'voided_column' the columns corresponding to a column number excluded at each step\n",
    "        df_test:  A test frame that contains the same columns for test data\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train_i = X_train_scaled\n",
    "    X_test_i  = X_test_scaled\n",
    "    model = model_choice(X_train_i, y_train, model_name, dict_of_params)\n",
    "\n",
    "    acc_train, f1_train = clf_performance(model, X_train_i, y_train)\n",
    "    acc_test, f1_test   = clf_performance(model, X_test_i, y_test)\n",
    "    \n",
    "    new_row_train = {'acc': [acc_train], 'f1': [f1_train], 'voided_column': [16]}\n",
    "    df_train = pd.DataFrame(new_row_train)\n",
    "    new_row_test  = {'acc': [acc_test],  'f1': [f1_test],  'voided_column': [16]}\n",
    "    df_test  = pd.DataFrame(new_row_test)\n",
    "\n",
    "    for i in range(X_train_scaled.shape[1] - 1):\n",
    "#    for i in range(3):\n",
    "        n = X_train_scaled.shape[1] - i\n",
    "#        print('i =', n)\n",
    "        acc_max = 0\n",
    "        f1_max  = 0\n",
    "        mod_max = 0\n",
    "        j_max   = 0\n",
    "        for j in range(n):\n",
    "    #        print(j)\n",
    "            X_train_j = np.delete(X_train_i, j, axis = 1)\n",
    "            model = model_choice(X_train_j, y_train, model_name, dict_of_params)\n",
    "            acc_train, f1_train = clf_performance(model, X_train_j, y_train)\n",
    "            if f1_train > f1_max:\n",
    "                f1_max  = f1_train\n",
    "                acc_max = acc_train\n",
    "                mod_max = model\n",
    "                j_max   = j\n",
    "        X_train_i = np.delete(X_train_i, j_max, axis = 1)\n",
    "        X_test_i  = np.delete(X_test_i,  j_max, axis = 1)\n",
    "        acc_test, f1_test = clf_performance(mod_max, X_test_i, y_test)\n",
    "\n",
    "        df_train.loc[len(df_train.index)] = [acc_max,  f1_max,  j_max]\n",
    "        df_test.loc[len(df_test.index)]   = [acc_test, f1_test, j_max]\n",
    "\n",
    "    df_train['voided_column'] = df_train['voided_column'].astype(int)\n",
    "    df_test['voided_column']  = df_test['voided_column'].astype(int)\n",
    "\n",
    "    return  df_train, df_test\n",
    "\n",
    "def features_list(features_list, j_list, n_features = 16, is_print = False):\n",
    "    \"\"\"\n",
    "    The function extracts the important feature list obtained by the function feature_extraction.\n",
    "\n",
    "    Args:\n",
    "        features_list: A list of all features.\n",
    "        j_list:        A list of feature numbers excluded at each step of the backward stepwise algorithm.\n",
    "        n_features:    A number of features in the output feature list.\n",
    "        is_print:      A variable equals True for the sets of features printing.\n",
    "\n",
    "    Returns:\n",
    "        return_list: A shrunk feature list.\n",
    "    \"\"\"\n",
    "    list_lenght = len(features_list)\n",
    "    if is_print == True:\n",
    "        print(features_list)\n",
    "\n",
    "    ind = list_lenght - n_features\n",
    "    return_list = features_list\n",
    "\n",
    "    N = len(features_list)\n",
    "    for i in range(N):\n",
    "        col_ind = j_list[i]\n",
    "        if col_ind < list_lenght:\n",
    "            features_list = np.delete(features_list, col_ind)\n",
    "            if is_print == True:\n",
    "                print(features_list)\n",
    "            if i == ind:\n",
    "                return_list = features_list\n",
    "    return return_list\n",
    "\n",
    "def get_matrices(n_features, X_train_scaled, X_test_scaled, j_list):\n",
    "    \"\"\"\n",
    "    The function extracts the matrices of the important feature obtained by the function feature_extraction.\n",
    "\n",
    "    Args:\n",
    "        n_features:     A number of features in the output feature list.\n",
    "        X_train_scaled: A scaled matrix of train data.\n",
    "        X_test_scaled:  A scaled matrix of test data.\n",
    "        j_list:         A list of feature numbers excluded at each step of the backward stepwise algorithm.\n",
    "\n",
    "    Returns:\n",
    "        X_train_i: A shrunk matrix of train data.\n",
    "        X_test_i:  A shrunk matrix of test data.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    X_train_i = X_train_scaled\n",
    "    X_test_i  = X_test_scaled\n",
    "    \n",
    "    for i in range(X_train_scaled.shape[1] - n_features + 1):\n",
    "        col_ind = j_list[i]\n",
    "        if col_ind < X_test_scaled.shape[1]:\n",
    "            X_train_i = np.delete(X_train_i, col_ind, axis = 1)\n",
    "            X_test_i  = np.delete(X_test_i, col_ind, axis = 1)\n",
    "    \n",
    "#    # Checking if the extraction of columns is correct\n",
    "#    print('X_test_scaled =')\n",
    "#    print(X_test_scaled[:3, :])\n",
    "#    print('X_test =')\n",
    "#    print(X_test_i[:3, :])\n",
    "\n",
    "    return X_train_i, X_test_i\n",
    "\n",
    "def get_performace_on_grid(X_train_scaled, y_train, X_test_scaled, y_test, x_range, model_name):\n",
    "    \"\"\"\n",
    "    The function extracts the matrices of the important feature obtained by the function feature_extraction.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled: A scaled matrix of train data.\n",
    "        y_train:        A train output.\n",
    "        X_test_scaled:  A scaled matrix of test data.\n",
    "        y_test:         A test output.\n",
    "        x_range:        A grid for a hyperparameter.\n",
    "        model_name:     A model mane.\n",
    "\n",
    "    Returns:\n",
    "        Acc_train_matr: A matrix of the training accuracy of each grid node.\n",
    "        F1_train_matr:  A matrix of the training F1 score of each grid node. \n",
    "        Acc_test_matr:  A matrix of the test accuracy of each grid node.\n",
    "        F1_test_matr:   A matrix of the test F1 score of each grid node. \n",
    "        Voided_col:     A matrix of voided columns for each number of features. \n",
    "    \"\"\"\n",
    "    for i in range(len(x_range)):\n",
    "        if model_name == 'KNN':\n",
    "            print('Number of neighbors:', x_range[i])\n",
    "            dict_of_params = {'n_neighbors': x_range[i]}\n",
    "        elif model_name == 'DT':\n",
    "            print('Max depth:', x_range[i])\n",
    "            dict_of_params = {'min_samples_leaf': 3, 'max_depth': x_range[i]}\n",
    "        elif model_name == 'SVC':\n",
    "            print('C:', x_range[i])\n",
    "            dict_of_params = {'C': x_range[i]}\n",
    "            \n",
    "        df_train, df_test = feature_extraction(X_train_scaled, y_train, X_test_scaled, y_test, model_name, dict_of_params)\n",
    "        \n",
    "        if i == 0:\n",
    "            Acc_train_matr = df_train['acc'].to_numpy()[::-1]\n",
    "            F1_train_matr  = df_train['f1'].to_numpy()[::-1]\n",
    "            Acc_test_matr  = df_test['acc'].to_numpy()[::-1]\n",
    "            F1_test_matr   = df_test['f1'].to_numpy()[::-1]\n",
    "            Voided_col     = df_train['voided_column'].to_numpy()\n",
    "            \n",
    "        else:\n",
    "            Acc_train_matr = np.column_stack((Acc_train_matr, df_train['acc'].to_numpy()[::-1]))\n",
    "            F1_train_matr  = np.column_stack((F1_train_matr,  df_train['f1'].to_numpy()[::-1]))\n",
    "            Acc_test_matr  = np.column_stack((Acc_test_matr,  df_test['acc'].to_numpy()[::-1]))\n",
    "            F1_test_matr   = np.column_stack((F1_test_matr,   df_test['f1'].to_numpy()[::-1]))\n",
    "            Voided_col     = np.column_stack((Voided_col,     df_test['voided_column'].to_numpy()))\n",
    "\n",
    "    return Acc_train_matr, F1_train_matr, Acc_test_matr, F1_test_matr, Voided_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d9731-5e61-4f4a-8b5b-f3f4d29dc059",
   "metadata": {},
   "source": [
    "#### Plot of a performance metric in a space of number of features and a chosen hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "416066e3-3ed6-4a7d-8aff-f98927f0842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(ax, metric, x_range, y_range, metric_name, param_name, is_error = False):\n",
    "    \"\"\"\n",
    "    The function plots a 3D performance metric in the space of a number of features and a number of neighbors in KNN.\n",
    "\n",
    "    Args:\n",
    "        ax: A subplot.\n",
    "        metric: A matrix of a performance metric.\n",
    "        x_range: An x-axis range.\n",
    "        y_range: A y-axis range.\n",
    "        metric_name: A name of a performance metric.\n",
    "        param_name: A hyperparameer name.\n",
    "        is_error: A variable should be True for construction of an Error \n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = metric\n",
    "    if is_error == True:\n",
    "        Z = np.ones(Z.shape) - Z\n",
    "    ax.plot_surface(X, Y, Z, cmap = 'viridis', alpha = 0.7)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel(param_name)\n",
    "    ax.set_ylabel('Features')\n",
    "    ax.set_title(metric_name)\n",
    "\n",
    "def plot_performance_metric(row, col, ind, train_metric, test_metric, x_range, metric_name, n_features, x_label, loc_pos):\n",
    "    \"\"\"\n",
    "    The function plots a 2D performance metric for the given number of features versus a number of neighbors in KNN.\n",
    "\n",
    "    Args:\n",
    "        row: A row of a subplot.\n",
    "        col: A column of a subplot.\n",
    "        ind: An index of a graph.\n",
    "        train_metric: A matrix of a performance metric obtained for the train data. \n",
    "        test_metric:  A matrix of a performance metric obtained for the test data.\n",
    "        x_range: A vector of x-axes labels\n",
    "        metric_name:  A name of a performance metric.\n",
    "        n_features:   A given number of features (fixed for the graph).\n",
    "        x_label: A name of x_axes.\n",
    "        loc_pos: A legend location (4 - right bottom, 5 - right center)\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    if metric_name == 'Error':\n",
    "        train_list = np.ones(train_metric.shape[1]) - train_metric[n_features][:]\n",
    "        test_list  = np.ones(test_metric.shape[1])  - test_metric[n_features][:]\n",
    "    else:\n",
    "        train_list = train_metric[n_features][:]\n",
    "        test_list  = test_metric[n_features][:]\n",
    "        \n",
    "#    positions = list(range(1, len(train_list) + 1))\n",
    "    positions = x_range\n",
    "    y_label = metric_name\n",
    "    \n",
    "    plt.subplot(row, col, ind)\n",
    "    plt.plot(positions, train_list, marker = 'o', linestyle = '-', color = 'b', label = 'Train ' + y_label)\n",
    "    plt.plot(positions, test_list, marker = 'o', linestyle = '-', color = 'g', label = 'Test ' + y_label)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    if n_features > 0:\n",
    "        plt.title(y_label + ' vs. ' + x_label + ' for ' + str(n_features) + ' Features')\n",
    "    else:\n",
    "        plt.title(y_label + ' vs. ' + x_label)\n",
    "    plt.legend(loc = loc_pos)  \n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be092944-03cc-4627-a39f-d55907894a63",
   "metadata": {},
   "source": [
    "## Getting Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe12b01-8b17-4761-a00d-f17f690838b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_performance(model, X, y, is_print = False, y_pred = np.array([])):\n",
    "    \"\"\"\n",
    "    The function calculates the performance of a classifier.\n",
    "\n",
    "    Args:\n",
    "        model: A model for the performance estimation (if no model, then y_pred is used). \n",
    "        X: A test feature matrix.\n",
    "        y: A test output vector.\n",
    "        is_print: If True, the results will be printed.\n",
    "        y_pred: A predicted output vector.\n",
    "\n",
    "    Returns:\n",
    "        acc: The accuracy\n",
    "        f1: F1-score.\n",
    "    \"\"\"\n",
    "    if y_pred.shape[0] == 0:\n",
    "        y_pred = model.predict(X)\n",
    "    f1 = f1_score(y, y_pred, average = 'macro')\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    if is_print == True:\n",
    "        print(f\"Accuracy = {acc:.4f}\")\n",
    "        print(f\"F1 = {f1:.4f}\")\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        print('Confusion matrix:')\n",
    "        print(cm)\n",
    "\n",
    "    return acc, f1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
